import json
import os
import sys
from datetime import datetime
from typing import TYPE_CHECKING
import cv2
import cv2.aruco as aruco
import numpy as np
import requests

if TYPE_CHECKING:
    import yomitoku.schemas

# Ensure Qt uses PySide6's plugins rather than OpenCV's bundled plugins which can cause
# "Could not load the Qt platform plugin 'xcb'" errors. We set QT_PLUGIN_PATH to PySide6
# package plugins directory when possible.
try:
    import PySide6
    from PySide6 import QtCore, QtGui, QtWidgets

    pyside_pkg_dir = os.path.dirname(PySide6.__file__)
    pyside_plugins = os.path.join(pyside_pkg_dir, "plugins")
    # Prepend to QT_PLUGIN_PATH so Qt finds PySide6 plugins first
    existing = os.environ.get("QT_PLUGIN_PATH", "")
    if pyside_plugins and pyside_plugins not in existing:
        os.environ["QT_PLUGIN_PATH"] = pyside_plugins + (
            os.pathsep + existing if existing else ""
        )
except Exception:
    # If PySide6 import fails, re-raise so the error is visible
    raise

from chatgpt import AIProcessingDialog
from config_loader import get_config
from image_processing import (
    auto_white_balance,
    calculate_marker_rotation,
    correct_rotation,
    draw_debug_grid,
    perspective_transform_from_marker,
)
from ocr_worker import YomiTokuWorker
from ui_components import SubjectSettingsDialog, ToastNotification


class CameraWindow(QtWidgets.QMainWindow):
    def __init__(self, debug_mode=False):
        super().__init__()

        # è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        self.config = get_config()

        self.setWindowTitle("Aruco + OCR Camera")
        self.resize(self.config.get_window_width(),
                    self.config.get_window_height())

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’è¨­å®š
        icon_path = os.path.join(os.path.dirname(__file__), "icon.png")
        if os.path.exists(icon_path):
            self.setWindowIcon(QtGui.QIcon(icon_path))

        # ãƒ¢ãƒ€ãƒ³ãªãƒ‡ã‚¶ã‚¤ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚·ãƒ¼ãƒˆã‚’é©ç”¨
        self.setStyleSheet(
            """
            QMainWindow {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #1a1a2e, stop:1 #16213e);
            }
            QLabel#videoLabel {
                background-color: #0f3460;
                border-radius: 12px;
                border: 2px solid #533483;
                padding: 8px;
            }
            QPushButton {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #533483, stop:1 #3d2564);
                color: white;
                border: none;
                border-radius: 8px;
                padding: 12px 24px;
                font-size: 14px;
                font-weight: normal;
                min-width: 100px;
            }
            QPushButton:hover {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #6b4397, stop:1 #533483);
            }
            QPushButton:pressed {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #3d2564, stop:1 #2d1a4c);
                padding-top: 14px;
            }
            QPushButton#settingsButton {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #e94560, stop:1 #c42847);
            }
            QPushButton#settingsButton:hover {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #ff5577, stop:1 #e94560);
            }
            QPushButton#quitButton {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #757575, stop:1 #5a5a5a);
            }
            QPushButton#quitButton:hover {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #8a8a8a, stop:1 #707070);
            }
            QPushButton#wbToggleOn {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #4CAF50, stop:1 #388E3C);
            }
            QPushButton#wbToggleOn:hover {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #66BB6A, stop:1 #4CAF50);
            }
            QPushButton#wbToggleOff {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #757575, stop:1 #5a5a5a);
            }
            QPushButton#wbToggleOff:hover {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #8a8a8a, stop:1 #707070);
            }
            QPushButton#resumeButton {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #2196F3, stop:1 #1976D2);
            }
            QPushButton#resumeButton:hover {
                background: #4CAF50;
            }

            QPushButton#aiButton {
                background: #9C27B0;
            }
            QPushButton#aiButton:hover {
                background: #7B1FA2;
            }
            QPushButton#aiButton:disabled {
                background: #666666;
                color: #999999;
            }
            QTextEdit {
                background-color: #16213e;
                color: #e0e0e0;
                border: 2px solid #533483;
                border-radius: 8px;
                padding: 12px;
                font-family: 'Consolas', 'Monaco', monospace;
                font-size: 13px;
                selection-background-color: #533483;
            }
            QTextEdit:focus {
                border: 2px solid #6b4397;
            }
        """
        )

        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
        self.debug_mode = debug_mode

        # captures directory
        self.captures_dir = os.path.join(
            os.path.dirname(__file__), self.config.get_captures_dir()
        )
        os.makedirs(self.captures_dir, exist_ok=True)

        # subject mappings JSON file
        self.settings_file = os.path.join(
            os.path.dirname(__file__), self.config.get_subject_mappings_file()
        )
        self.subject_mappings = self.load_subject_mappings()

        # Video capture: try network MJPEG stream first, but verify we can actually read a frame.
        # If the stream can't provide frames, fall back to the local camera (index 0).
        def try_open_capture(source, tries=3):
            cap = cv2.VideoCapture(source)
            if not cap.isOpened():
                return None
            # quick read-check: attempt to read one frame (with small flush attempts)
            for _ in range(tries):
                ret, _ = cap.read()
                if ret:
                    return cap
            # no frames read -> treat as failure
            try:
                cap.release()
            except Exception:
                pass
            return None

        # è¨­å®šã‹ã‚‰ã‚«ãƒ¡ãƒ©ã‚¿ã‚¤ãƒ—ã¨URLã‚’å–å¾—
        self.cap = try_open_capture(
            self.config.get_network_video_url(),
            tries=self.config.get_network_retry_count(),
        )
        self.cap_type = "network"
        if self.cap is None:
            # try the default local camera
            self.cap = try_open_capture(
                self.config.get_local_device_index(),
                tries=self.config.get_network_retry_count(),
            )
            self.cap_type = "local"

        if self.cap is None:
            # show a user-friendly error and stop initialization
            QtWidgets.QMessageBox.critical(
                self,
                "ã‚¨ãƒ©ãƒ¼",
                "ã‚«ãƒ¡ãƒ©ã‚’é–‹ãã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚«ãƒ¡ãƒ©ã¨ãƒ­ãƒ¼ã‚«ãƒ«ã‚«ãƒ¡ãƒ©ã®ä¸¡æ–¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                QtWidgets.QMessageBox.StandardButton.Ok,
                QtWidgets.QMessageBox.StandardButton.Ok,
            )
            # raise an exception so caller can handle it (or exit in main)
            raise RuntimeError("Failed to open any camera source")

        # Set buffer size to 1 to always get the latest frame and prevent latency buildup
        # This is critical when stream FPS > processing FPS (e.g., 60fps stream with 33fps timer)
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, self.config.get_buffer_size())

        # self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
        # self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)
        # self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 3840)
        # self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)
        # self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('H', '2', '6', '4'))
        # self.cap.set(cv2.CAP_PROP_FPS, 30)

        # ArUco setup
        # è¨­å®šã‹ã‚‰ArUcoè¾æ›¸ã‚¿ã‚¤ãƒ—ã‚’å–å¾—
        dict_type_name = self.config.get_aruco_dict_type()
        dict_type = getattr(aruco, dict_type_name, aruco.DICT_4X4_50)
        self.aruco_dict = aruco.getPredefinedDictionary(dict_type)
        params = aruco.DetectorParameters()
        self.detector = aruco.ArucoDetector(self.aruco_dict, params)

        # ArUco ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®é–¾å€¤ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        # - aruco_area_ratio_threshold: ç”»åƒé¢ç©ã«å¯¾ã™ã‚‹ãƒãƒ¼ã‚«ãƒ¼é¢ç©ã®æ¯”ç‡ï¼ˆå°ã•ã™ãã‚‹ã‚‚ã®ã‚’é™¤å¤–ï¼‰
        # - aruco_fill_threshold: ãƒãƒ¼ã‚«ãƒ¼å‡¸åŒ…ã«å¯¾ã™ã‚‹å®Ÿéš›ã®ãƒãƒªã‚´ãƒ³é¢ç©ã®å……å¡«ç‡ï¼ˆæ­ªã¿åˆ¤å®šï¼‰
        self.aruco_area_ratio_threshold = self.config.get_aruco_area_ratio_threshold()
        self.aruco_fill_threshold = self.config.get_aruco_fill_threshold()

        # ----- UI ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— -----
        # ãƒ¡ã‚¤ãƒ³ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œã‚‹
        # (ãƒ“ãƒ‡ã‚ªè¡¨ç¤ºãƒ©ãƒ™ãƒ«ã€ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³ç¾¤ã€OCR å‡ºåŠ›)
        # UI éƒ¨åˆ†ã¯å¾Œã»ã© update_frame() ã§ç”»åƒã‚’ QLabel ã«æµã—è¾¼ã¿ã¾ã™
        # -----
        # ãƒ¡ã‚¤ãƒ³ã®ã‚­ãƒ£ãƒ—ãƒãƒ£ç”»é¢ã‚’ä¿æŒã—ã¦ãŠãã€AIç”»é¢ã¨åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
        self.camera_central = QtWidgets.QWidget()
        layout = QtWidgets.QVBoxLayout(self.camera_central)

        # ã‚«ãƒ¡ãƒ©æ˜ åƒã‚’è¡¨ç¤ºã™ã‚‹ QLabel
        self.video_label = QtWidgets.QLabel()
        # æœ€å°ã‚µã‚¤ã‚ºã‚’è¨­å®šã—ã¦ãŠãï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ç¸®å°ã§æ½°ã‚Œã™ããªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
        self.video_label.setMinimumSize(
            self.config.get_video_label_min_width(),
            self.config.get_video_label_min_height(),
        )
        self.video_label.setStyleSheet("background-color: black;")
        layout.addWidget(self.video_label)

        controls = QtWidgets.QHBoxLayout()
        controls.setSpacing(12)
        layout.addLayout(controls)

        # æ•™ç§‘è¨­å®šãƒœã‚¿ãƒ³
        self.settings_btn = QtWidgets.QPushButton("âš™ï¸ æ•™ç§‘è¨­å®š")
        self.settings_btn.setObjectName("settingsButton")
        self.settings_btn.clicked.connect(self.open_subject_settings)
        self.settings_btn.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)
        controls.addWidget(self.settings_btn)

        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒãƒ©ãƒ³ã‚¹è£œæ­£ãƒˆã‚°ãƒ«ãƒœã‚¿ãƒ³
        wb_default = self.config.get_white_balance_enabled_by_default()
        self.wb_toggle_btn = QtWidgets.QPushButton(
            "è£œæ­£ON" if wb_default else "è£œæ­£OFF"
        )
        self.wb_toggle_btn.setCheckable(True)
        self.wb_toggle_btn.setChecked(wb_default)
        self.wb_toggle_btn.clicked.connect(self.toggle_white_balance)
        self.settings_btn.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)
        controls.addWidget(self.wb_toggle_btn)

        # ArUco æ¤œå‡ºã‚’ãƒˆãƒªã‚¬ãƒ¼ã«è‡ªå‹•æ’®å½±ã™ã‚‹ãŸã‚ã®å˜ç™ºã‚¿ã‚¤ãƒãƒ¼
        # ãƒãƒ¼ã‚«ãƒ¼ã‚’æ¤œå‡ºã—ãŸã‚‰ã“ã®ã‚¿ã‚¤ãƒãƒ¼ã‚’ start ã—ã¦ä¸€å®šæ™‚é–“ï¼ˆcapture_delay_msï¼‰å¾Œã«æ’®å½±ã™ã‚‹
        self.capture_delay_ms = self.config.get_aruco_auto_capture_delay_ms()
        self.aruco_auto_timer = QtCore.QTimer(self)
        self.aruco_auto_timer.setSingleShot(True)
        self.aruco_auto_timer.timeout.connect(self.take_picture)
        # å‰ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ¤œå‡ºçŠ¶æ…‹ã‚’ä¿æŒã—ã¦ã€çŠ¶æ…‹é·ç§»ã§ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹/åœæ­¢ã™ã‚‹
        self._last_aruco_detected = False

        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒãƒ©ãƒ³ã‚¹è£œæ­£ã®ON/OFFåˆ‡ã‚Šæ›¿ãˆãƒ•ãƒ©ã‚°
        self.white_balance_enabled = wb_default

        spacer = QtWidgets.QSpacerItem(
            40,
            20,
            QtWidgets.QSizePolicy.Policy.Expanding,
            QtWidgets.QSizePolicy.Policy.Minimum,
        )

        controls.addItem(spacer)

        # æ’®å½±å¾Œã«ä¸€æ™‚åœæ­¢ã—ãŸãƒ©ã‚¤ãƒ–ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å†é–‹ã™ã‚‹ãƒœã‚¿ãƒ³
        self.resume_btn = QtWidgets.QPushButton("ğŸ“· æ’®å½±å†é–‹")
        self.resume_btn.setObjectName("resumeButton")
        self.resume_btn.clicked.connect(self.resume_camera)
        self.resume_btn.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)
        controls.addWidget(self.resume_btn)

        # OCR ã®çµæœãªã©ã‚’è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸ
        self.ocr_output = QtWidgets.QTextEdit()
        self.ocr_output.setReadOnly(True)
        self.ocr_output.setMaximumHeight(
            self.config.get_ocr_output_max_height())
        self.ocr_output.setPlaceholderText("OCRçµæœãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™...")
        layout.addWidget(self.ocr_output)

        # --- AIç”»é¢ã‚’ä½œæˆ (ç”»é¢é·ç§»ç”¨) ---
        self.ai_page = QtWidgets.QWidget()
        ai_layout = QtWidgets.QVBoxLayout(self.ai_page)
        ai_layout.setContentsMargins(20, 20, 20, 20)
        ai_layout.setSpacing(12)

        ai_title = QtWidgets.QLabel("ğŸ“š éå»ã®æ’®å½±å±¥æ­´")
        ai_title.setStyleSheet(
            "font-size:18px; color: #e0e0e0; font-weight: bold;")
        ai_layout.addWidget(ai_title)

        # æ°´å¹³åˆ†å‰²: å·¦å´ã«ãƒªã‚¹ãƒˆã€å³å´ã«è©³ç´°è¡¨ç¤º
        ai_splitter = QtWidgets.QSplitter(QtCore.Qt.Orientation.Horizontal)

        # å·¦å´: æ’®å½±å±¥æ­´ãƒªã‚¹ãƒˆ
        left_widget = QtWidgets.QWidget()
        left_layout = QtWidgets.QVBoxLayout(left_widget)
        left_layout.setContentsMargins(0, 0, 0, 0)

        # æ•™ç§‘ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        filter_layout = QtWidgets.QHBoxLayout()
        filter_label = QtWidgets.QLabel("æ•™ç§‘ã§çµã‚Šè¾¼ã¿:")
        filter_label.setStyleSheet("color: #e0e0e0;")
        filter_layout.addWidget(filter_label)

        self.subject_filter = QtWidgets.QComboBox()
        self.subject_filter.setStyleSheet("""
            QComboBox {
                background-color: #16213e;
                color: #e0e0e0;
                border: 2px solid #533483;
                border-radius: 4px;
                padding: 4px;
            }
            QComboBox:hover {
                border: 2px solid #6b4397;
            }
            QComboBox::drop-down {
                border: none;
            }
            QComboBox QAbstractItemView {
                background-color: #16213e;
                color: #e0e0e0;
                selection-background-color: #533483;
            }
        """)
        self.subject_filter.currentTextChanged.connect(
            self.filter_captures_by_subject)
        filter_layout.addWidget(self.subject_filter)
        filter_layout.addStretch()
        left_layout.addLayout(filter_layout)

        # æ’®å½±å±¥æ­´ãƒªã‚¹ãƒˆ
        self.capture_list = QtWidgets.QListWidget()
        self.capture_list.setStyleSheet("""
            QListWidget {
                background-color: #16213e;
                color: #e0e0e0;
                border: 2px solid #533483;
                border-radius: 8px;
                padding: 4px;
                font-size: 13px;
            }
            QListWidget::item {
                padding: 8px;
                border-bottom: 1px solid #533483;
            }
            QListWidget::item:selected {
                background-color: #533483;
            }
            QListWidget::item:hover {
                background-color: #3d2564;
            }
        """)
        self.capture_list.itemSelectionChanged.connect(
            self.on_capture_selected)
        left_layout.addWidget(self.capture_list)

        ai_splitter.addWidget(left_widget)

        # å³å´: è©³ç´°è¡¨ç¤º
        right_widget = QtWidgets.QWidget()
        right_layout = QtWidgets.QVBoxLayout(right_widget)
        right_layout.setContentsMargins(0, 0, 0, 0)

        # ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        self.preview_label = QtWidgets.QLabel("ç”»åƒã‚’é¸æŠã—ã¦ãã ã•ã„")
        self.preview_label.setAlignment(QtCore.Qt.AlignmentFlag.AlignCenter)
        self.preview_label.setStyleSheet("""
            QLabel {
                background-color: #0f3460;
                border: 2px solid #533483;
                border-radius: 8px;
                color: #808080;
                min-height: 300px;
            }
        """)
        self.preview_label.setScaledContents(False)
        right_layout.addWidget(self.preview_label, stretch=3)

        # OCRãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
        self.ai_text_display = QtWidgets.QTextEdit()
        self.ai_text_display.setReadOnly(True)
        self.ai_text_display.setPlaceholderText("OCRçµæœãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™")
        right_layout.addWidget(self.ai_text_display, stretch=2)

        # AIå‡¦ç†ãƒœã‚¿ãƒ³
        ai_process_detail_btn = QtWidgets.QPushButton("ğŸ¤– AIå‡¦ç†ã‚’å®Ÿè¡Œ")
        ai_process_detail_btn.setObjectName("aiButton")
        ai_process_detail_btn.clicked.connect(self.open_ai_processing)
        ai_process_detail_btn.setCursor(
            QtCore.Qt.CursorShape.PointingHandCursor)
        right_layout.addWidget(ai_process_detail_btn)

        ai_splitter.addWidget(right_widget)
        ai_splitter.setStretchFactor(0, 1)
        ai_splitter.setStretchFactor(1, 2)

        ai_layout.addWidget(ai_splitter)

        # ä¸­å¤®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã¯ QStackedWidget ã‚’ä½¿ã£ã¦ç”»é¢é·ç§»ã‚’è¡Œã†
        # ï¼ˆsetCentralWidget ã§ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã¨å¤ã„ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆãŒ
        #  å‰Šé™¤ã•ã‚Œã¦ã—ã¾ã„ã€C++ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒæ—¢ã«å‰Šé™¤ã•ã‚Œã‚‹å•é¡ŒãŒç™ºç”Ÿã™ã‚‹ãŸã‚ï¼‰

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠï¼ˆä¸Šéƒ¨ã«ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ãƒœã‚¿ãƒ³ã€ä¸‹éƒ¨ã«ã‚¹ã‚¿ãƒƒã‚¯ï¼‰
        main_container = QtWidgets.QWidget()
        main_layout = QtWidgets.QVBoxLayout(main_container)
        main_layout.setContentsMargins(0, 0, 0, 0)
        main_layout.setSpacing(0)

        # ä¸Šéƒ¨: ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ãƒãƒ¼
        mode_bar = QtWidgets.QWidget()
        mode_bar.setStyleSheet("""
            QWidget {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #533483, stop:1 #3d2564);
                border-bottom: 2px solid #6b4397;
            }
        """)
        mode_bar_layout = QtWidgets.QHBoxLayout(mode_bar)
        mode_bar_layout.setContentsMargins(10, 5, 10, 5)
        mode_bar_layout.setSpacing(10)

        # ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ãƒœã‚¿ãƒ³
        self.camera_mode_btn = QtWidgets.QPushButton("ğŸ“· æ’®å½±ãƒ¢ãƒ¼ãƒ‰")
        self.camera_mode_btn.setCheckable(True)
        self.camera_mode_btn.setChecked(True)
        self.camera_mode_btn.clicked.connect(self.show_camera_page)
        self.camera_mode_btn.setCursor(
            QtCore.Qt.CursorShape.PointingHandCursor)
        self.camera_mode_btn.setStyleSheet("""
            QPushButton {
                background-color: #2196F3;
                color: white;
                border: none;
                border-radius: 4px;
                padding: 8px 16px;
                font-size: 13px;
                font-weight: bold;
            }
            QPushButton:checked {
                background-color: #1976D2;
                border: 2px solid #64B5F6;
            }
            QPushButton:hover {
                background-color: #42A5F5;
            }
        """)
        mode_bar_layout.addWidget(self.camera_mode_btn)

        self.ai_mode_btn = QtWidgets.QPushButton("ğŸ“š AIãƒ¢ãƒ¼ãƒ‰")
        self.ai_mode_btn.setCheckable(True)
        self.ai_mode_btn.setChecked(False)
        self.ai_mode_btn.clicked.connect(self.show_ai_page)
        self.ai_mode_btn.setCursor(QtCore.Qt.CursorShape.PointingHandCursor)
        self.ai_mode_btn.setStyleSheet("""
            QPushButton {
                background-color: #9C27B0;
                color: white;
                border: none;
                border-radius: 4px;
                padding: 8px 16px;
                font-size: 13px;
                font-weight: bold;
            }
            QPushButton:checked {
                background-color: #7B1FA2;
                border: 2px solid #CE93D8;
            }
            QPushButton:hover {
                background-color: #AB47BC;
            }
        """)
        mode_bar_layout.addWidget(self.ai_mode_btn)
        # ãƒœã‚¿ãƒ³ã‚µã‚¤ã‚ºã‚’çµ±ä¸€ï¼ˆé«˜ã•ã¨æœ€å°å¹…ï¼‰
        uniform_height = 36
        uniform_min_width = 120
        try:
            self.camera_mode_btn.setFixedHeight(uniform_height)
            self.camera_mode_btn.setMinimumWidth(uniform_min_width)
            self.ai_mode_btn.setFixedHeight(uniform_height)
            self.ai_mode_btn.setMinimumWidth(uniform_min_width)
        except Exception:
            pass

        mode_bar_layout.addStretch()

        main_layout.addWidget(mode_bar)

        # ã‚¹ã‚¿ãƒƒã‚¯
        self.stack = QtWidgets.QStackedWidget()
        self.stack.addWidget(self.camera_central)
        self.stack.addWidget(self.ai_page)
        main_layout.addWidget(self.stack)

        self.setCentralWidget(main_container)

        # åˆæœŸã¯ã‚«ãƒ¡ãƒ©ç”»é¢ã‚’è¡¨ç¤º
        self.stack.setCurrentWidget(self.camera_central)

        # ã‚«ãƒ¡ãƒ©ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å®šæœŸçš„ã«å–å¾—ã—ã¦è¡¨ç¤ºã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒãƒ¼
        # å¤§ä½“ 30ms ã”ã¨ï¼ˆç´„33fpsï¼‰ã§ update_frame ã‚’å‘¼ã¶
        self.timer = QtCore.QTimer(self)
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(self.config.get_frame_interval_ms())

        # remove automatic OCR timer: OCR should run after taking a picture
        # (the OCR button will still be available for manual runs)

        self.current_frame = None

        # Flag to pause camera feed display (but keep reading frames to maintain stream sync)
        self.camera_paused = False
        self.paused_display_frame = None

        # æœ€å¾Œã®OCRçµæœã‚’ä¿å­˜ï¼ˆAIå‡¦ç†ç”¨ï¼‰
        self.last_ocr_text = ""
        self.last_subject_name = ""

        self.ocr_timer: QtCore.QTimer = QtCore.QTimer(self)

    def load_subject_mappings(self):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ•™ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã‚€"""
        if os.path.exists(self.settings_file):
            try:
                with open(self.settings_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                QtWidgets.QMessageBox.warning(
                    self,
                    "è­¦å‘Š",
                    f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}",
                    QtWidgets.QMessageBox.StandardButton.Ok,
                    QtWidgets.QMessageBox.StandardButton.Ok,
                )
                return {}
        return {}

    def save_subject_mappings(self):
        """æ•™ç§‘ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(self.settings_file, "w", encoding="utf-8") as f:
                json.dump(self.subject_mappings, f,
                          ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            QtWidgets.QMessageBox.critical(
                self,
                "ã‚¨ãƒ©ãƒ¼",
                f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}",
                QtWidgets.QMessageBox.StandardButton.Ok,
                QtWidgets.QMessageBox.StandardButton.Ok,
            )
            return False

    def open_subject_settings(self):
        """æ•™ç§‘è¨­å®šãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹ã"""
        dialog = SubjectSettingsDialog(self.subject_mappings, self)
        if dialog.exec() == QtWidgets.QDialog.DialogCode.Accepted:
            self.subject_mappings = dialog.get_mappings()
            if self.save_subject_mappings():
                toast = ToastNotification("æ•™ç§‘è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ", self, duration=4000)
                toast.show()

    def toggle_white_balance(self):
        """ãƒ›ãƒ¯ã‚¤ãƒˆãƒãƒ©ãƒ³ã‚¹è£œæ­£ã®ON/OFFã‚’åˆ‡ã‚Šæ›¿ãˆ"""
        self.white_balance_enabled = self.wb_toggle_btn.isChecked()
        if self.white_balance_enabled:
            self.wb_toggle_btn.setText("âœ“ è£œæ­£ON")
            self.wb_toggle_btn.setObjectName("wbToggleOn")
            toast = ToastNotification("ãƒ›ãƒ¯ã‚¤ãƒˆãƒãƒ©ãƒ³ã‚¹è£œæ­£: ON", self, duration=2000)
        else:
            self.wb_toggle_btn.setText("è£œæ­£OFF")
            self.wb_toggle_btn.setObjectName("wbToggleOff")
            toast = ToastNotification("ãƒ›ãƒ¯ã‚¤ãƒˆãƒãƒ©ãƒ³ã‚¹è£œæ­£: OFF", self, duration=2000)
        # ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å†é©ç”¨
        self.wb_toggle_btn.setStyle(self.wb_toggle_btn.style())
        toast.show()

    def update_frame(self):
        try:
            if self.cap is None:
                return
            ret, frame = self.cap.read()
            if not ret:
                return

            # Grab extra frames to flush buffer if stream FPS > timer FPS
            # This ensures we're always processing the most recent frame
            # and prevents latency buildup
            for _ in range(2):  # flush up to 2 old frames
                ret_flush, _ = self.cap.read()
                if not ret_flush:
                    break

        except Exception as e:
            # FFmpeg/MJPEG stream errors (e.g., "Stream ends prematurely", "overread")
            # These are often non-fatal, so we log and continue
            print(f"Warning: Frame read error: {e}")
            return

        # print(f"Captured frame size: {frame.shape[1]}x{frame.shape[0]}")

        # keep original BGR for saving/ocr
        self.current_frame = frame.copy()

        # If camera is paused, show the paused frame instead but keep reading to maintain stream sync
        if self.camera_paused:
            if self.paused_display_frame is not None:
                # Display the paused frame. The QLabel may have been deleted
                # (e.g. due to central widget swap), so guard against RuntimeError.
                try:
                    rgb = cv2.cvtColor(
                        self.paused_display_frame, cv2.COLOR_BGR2RGB)
                    h, w, ch = rgb.shape
                    bytes_per_line = ch * w
                    qimg = QtGui.QImage(
                        rgb.data, w, h, bytes_per_line, QtGui.QImage.Format.Format_RGB888
                    )
                    pix = QtGui.QPixmap.fromImage(qimg)
                    try:
                        label_size = self.video_label.size()
                    except RuntimeError:
                        return
                    if not pix.isNull():
                        pix = pix.scaled(
                            label_size, QtCore.Qt.AspectRatioMode.KeepAspectRatio
                        )
                        try:
                            self.video_label.setPixmap(pix)
                        except RuntimeError:
                            return
                except RuntimeError:
                    # Underlying Qt object was deleted; nothing to do
                    return
            return

        # ArUco ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡º: ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã§æ¤œå‡ºã‚’è¡Œã„ã€æ¤œå‡ºãŒã‚ã‚Œã°ãƒãƒ¼ã‚«ãƒ¼å€™è£œã‚’ãƒ•ã‚£ãƒ«ã‚¿
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        corners, ids, _ = self.detector.detectMarkers(gray)

        # corners/ids ã¯ None ã®å ´åˆã‚„ ndarray/list ã®å ´åˆãŒã‚ã‚‹ãŸã‚å®‰å…¨ã«å‡¦ç†ã™ã‚‹
        filtered_corners = []
        filtered_ids = []
        if ids is not None and len(ids) > 0:
            # image area used for normalization
            img_h, img_w = frame.shape[:2]
            img_area = float(img_h * img_w)
            for ci, cid in zip(
                corners, ids.flatten() if hasattr(ids, "flatten") else ids
            ):
                # corners ã®å½¢çŠ¶ã¯ (1,4,2) ã‚„ (4,2) ã®ã“ã¨ãŒã‚ã‚‹
                pts = ci.reshape(-1, 2)

                # polygon é¢ç©ã‚’è¨ˆç®—
                area = abs(cv2.contourArea(pts))
                # bounding rect ã¨é¢ç©ã®æ¯”ï¼ˆè©°ã¾ã‚Šå…·åˆï¼‰
                x, y, w, h = cv2.boundingRect(pts.astype(int))
                rect_area = float(w * h) if w > 0 and h > 0 else 0.0
                fill_ratio = (area / rect_area) if rect_area > 0 else 0.0
                area_ratio = area / img_area if img_area > 0 else 0.0

                # å˜ç´”ãªä¿¡é ¼åº¦åˆ¤å®š: é¢ç©ãŒååˆ†ã§ã‚ã‚Šï¼ˆç”»é¢ã«å¯¾ã—ã¦å°ã•ã™ããªã„ï¼‰ã€
                # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°çŸ©å½¢ã«æ¯”è¼ƒã—ã¦ãƒãƒªã‚´ãƒ³ãŒæ¥µç«¯ã«ç´°é•·/æ­ªã‚“ã§ã„ãªã„ã“ã¨
                if (
                    area_ratio >= self.aruco_area_ratio_threshold
                    and fill_ratio >= self.aruco_fill_threshold
                ):
                    filtered_corners.append(pts.reshape(1, -1, 2))
                    filtered_ids.append([cid])

        # aruco_detected ã¯ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®çµæœã‚’è¦‹ã‚‹
        aruco_detected = len(filtered_ids) > 0
        if aruco_detected:
            # è¡¨ç¤ºç”¨ã« OpenCV ã® drawDetectedMarkers ãŒæœŸå¾…ã™ã‚‹å½¢ã«æˆ»ã™
            try:
                fc = [c.astype(float) for c in filtered_corners]
                fid = (
                    cv2.UMat(cv2.UMat(np.array(filtered_ids))
                             ).get() if False else None
                )
            except Exception:
                # æœ€å°é™: use filtered_corners and filtered_ids directly
                pass
            # ç›´æ¥æç”»ã§ãã‚‹å½¢ã«ã™ã‚‹
            try:
                # filtered_corners ã¯ list of (1,4,2) ã«ãªã£ã¦ã„ã‚‹ã®ã§ãã®ã¾ã¾æ¸¡ã™
                aruco.drawDetectedMarkers(
                    frame, filtered_corners, np.array(filtered_ids)
                )
            except Exception:
                # fallback: draw original markers for visualization if conversion fails
                try:
                    aruco.drawDetectedMarkers(frame, corners, ids)
                except Exception:
                    pass

        # ãƒãƒ¼ã‚«ãƒ¼ã®æ¤œå‡ºçŠ¶æ…‹ã«å¿œã˜ã¦è‡ªå‹•æ’®å½±ã‚¿ã‚¤ãƒãƒ¼ã‚’åˆ¶å¾¡ã™ã‚‹
        # - ãƒãƒ¼ã‚«ãƒ¼ãŒæ–°ãŸã«æ¤œå‡ºã•ã‚ŒãŸã‚‰ å˜ç™ºã‚¿ã‚¤ãƒãƒ¼ã§è‡ªå‹•æ’®å½±ã‚’è¡Œã†
        # - ãƒãƒ¼ã‚«ãƒ¼ãŒæ¶ˆãˆãŸã‚‰ ä¿ç•™ä¸­ã®è‡ªå‹•æ’®å½±ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹
        # Note: only start auto-capture if camera is not paused
        if aruco_detected and not self._last_aruco_detected and not self.camera_paused:
            if not self.aruco_auto_timer.isActive():
                self.aruco_auto_timer.start(self.capture_delay_ms)
        elif aruco_detected:
            pass
        else:
            if self.aruco_auto_timer.isActive():
                self.aruco_auto_timer.stop()

        # æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ ç”¨ã«æ¤œå‡ºçŠ¶æ…‹ã‚’ä¿æŒ
        self._last_aruco_detected = aruco_detected

        # convert to RGB QImage
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb.shape
        bytes_per_line = ch * w
        qimg = QtGui.QImage(
            rgb.data, w, h, bytes_per_line, QtGui.QImage.Format.Format_RGB888
        )
        pix = QtGui.QPixmap.fromImage(qimg)
        # scale to label size â€” ensure label still exists
        try:
            label_size = self.video_label.size()
        except RuntimeError:
            # QLabel deleted (central widget swapped) â€” skip updating
            return
        if not pix.isNull():
            try:
                pix = pix.scaled(
                    label_size, QtCore.Qt.AspectRatioMode.KeepAspectRatio)
                self.video_label.setPixmap(pix)
            except RuntimeError:
                return

    def take_picture(self):
        if self.cap_type == "network":
            url = self.config.get_network_photo_url()
            response = requests.get(url)
            if response.status_code != 200:
                print("ç”»åƒã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return
            # ãƒã‚¤ãƒˆåˆ—ã‚’NumPyé…åˆ—ã«å¤‰æ›
            img_array = np.frombuffer(response.content, dtype=np.uint8)

            # OpenCVã§ãƒ‡ã‚³ãƒ¼ãƒ‰
            self.current_frame = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ã‚«ãƒ¡ãƒ©ã®å ´åˆã¯ç¾åœ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½¿ç”¨
            if self.current_frame is None:
                print("ç¾åœ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
                return

        if self.current_frame is None:
            return

        # æ¤œå‡ºã•ã‚ŒãŸãƒãƒ¼ã‚«ãƒ¼IDã‚’å–å¾—
        gray = cv2.cvtColor(self.current_frame, cv2.COLOR_BGR2GRAY)
        corners, ids, _ = self.detector.detectMarkers(gray)

        # ãƒãƒ¼ã‚«ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆ
        if ids is None or len(ids) == 0:
            QtWidgets.QMessageBox.warning(
                self,
                "è­¦å‘Š",
                "ArUcoãƒãƒ¼ã‚«ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                QtWidgets.QMessageBox.StandardButton.Ok,
                QtWidgets.QMessageBox.StandardButton.Ok,
            )
            return

        # æœ€åˆã«æ¤œå‡ºã•ã‚ŒãŸãƒãƒ¼ã‚«ãƒ¼IDã‚’ä½¿ç”¨
        marker_id = str(ids[0][0])

        # ãƒãƒ¼ã‚«ãƒ¼IDã«å¯¾å¿œã™ã‚‹æ•™ç§‘åã‚’å–å¾—
        subject_name = self.subject_mappings.get(marker_id, "æœªåˆ†é¡")

        # æ•™ç§‘åã‚’ä¿å­˜ï¼ˆAIå‡¦ç†ç”¨ï¼‰
        self.last_subject_name = subject_name

        # æ•™ç§‘ã”ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        subject_dir = os.path.join(self.captures_dir, subject_name)
        os.makedirs(subject_dir, exist_ok=True)

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆï¼ˆå…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å…±é€šï¼‰
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ã‚¹ãƒ†ãƒƒãƒ—0: å…ƒã®ç”»åƒã‚’ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰
        if self.debug_mode:
            original_filename = os.path.join(
                subject_dir, f"capture_{ts}_0_original.jpg"
            )
            cv2.imwrite(original_filename, self.current_frame)

        # ã‚¹ãƒ†ãƒƒãƒ—1: å°å½¢è£œæ­£ï¼ˆé€è¦–å¤‰æ›ï¼‰ã‚’é©ç”¨
        # ãƒãƒ¼ã‚«ãƒ¼ã®å››éš…ã®åº§æ¨™ã‹ã‚‰ç”»åƒå…¨ä½“ã‚’æ­£é¢ã‹ã‚‰è¦‹ãŸçŠ¶æ…‹ã«å¤‰æ›
        (
            perspective_frame,
            transform_matrix,
            output_size,
            corner_coords,
        ) = perspective_transform_from_marker(
            self.current_frame,
            np.asarray(corners),
            marker_size_mm=self.config.get_aruco_marker_size_mm(),
            output_dpi=self.config.get_aruco_output_dpi(),
        )

        # å°å½¢è£œæ­£ãŒæˆåŠŸã—ãŸå ´åˆã¯ãã®ç”»åƒã‚’ä½¿ç”¨ã€å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ç”»åƒã‚’ä½¿ç”¨
        if perspective_frame is not None:
            processing_frame = perspective_frame
            perspective_applied = True
            # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: å°å½¢è£œæ­£å¾Œã®ç”»åƒã‚’ä¿å­˜
            if self.debug_mode:
                perspective_filename = os.path.join(
                    subject_dir, f"capture_{ts}_1_perspective.jpg"
                )
                cv2.imwrite(perspective_filename, perspective_frame)
        else:
            processing_frame = self.current_frame.copy()
            perspective_applied = False

        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚„ã‚ã€æ¤œå‡ºã•ã‚ŒãŸå››è§’å½¢ã‚’æç”»ã—ã€ãƒãƒ•å¤‰æ›ã§ç›´ç·šã‚‚æç”»ã™ã‚‹
        # (å‡¦ç†ç”¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¯ãã®ã¾ã¾ä½¿ç”¨ã—ã€æç”»ã¯ã‚³ãƒ”ãƒ¼ä¸Šã§è¡Œã†)
        overlay = processing_frame.copy()

        # ã‚¨ãƒƒã‚¸æ¤œå‡ºã¨è¼ªéƒ­æ¤œå‡ºã«ã‚ˆã‚‹ç”¨ç´™ã®æ¤œå‡ºï¼ˆãƒˆãƒªãƒŸãƒ³ã‚°ã¯è¡Œã‚ãªã„ï¼‰
        gray_trim = cv2.cvtColor(processing_frame, cv2.COLOR_BGR2GRAY)
        kernel = tuple(self.config.get_gaussian_blur_kernel())
        blur = cv2.GaussianBlur(gray_trim, kernel, 0)
        edges = cv2.Canny(
            blur, self.config.get_canny_threshold1(), self.config.get_canny_threshold2()
        )

        # è¼ªéƒ­æ¤œå‡º
        contours, _ = cv2.findContours(
            edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )
        contours = sorted(contours, key=cv2.contourArea, reverse=True)

        paper_corners = None
        for cnt in contours:
            approx = cv2.approxPolyDP(
                cnt, 0.02 * cv2.arcLength(cnt, True), True)
            if len(approx) == 4:
                paper_corners = approx.reshape(4, 2)
                break

        # æ¤œå‡ºã•ã‚ŒãŸå››è§’å½¢ã‚’æç”»
        if paper_corners is not None:
            pts = paper_corners.astype(int)
            # ç·šã‚’æã
            cv2.polylines(overlay, [pts], isClosed=True,
                          color=(0, 255, 0), thickness=3)
            # å››éš…ã«å°ã•ã„å††ã‚’æç”»
            for x, y in pts:
                cv2.circle(overlay, (int(x), int(y)), 6, (0, 255, 0), -1)

        # ãƒãƒ•å¤‰æ›ã§ç›´ç·šæ¤œå‡º
        try:
            lines = cv2.HoughLinesP(
                edges,
                1,
                np.pi / 180,
                threshold=self.config.get_hough_threshold(),
                minLineLength=self.config.get_hough_min_line_length(),
                maxLineGap=self.config.get_hough_max_line_gap(),
            )
        except Exception:
            lines = None

        if lines is not None:
            lines = np.asarray(lines, dtype=np.int32)
            for l in lines:
                x1, y1, x2, y2 = l[0]
                # è§’åº¦ã‚’è¨ˆç®—ï¼ˆåº¦å˜ä½ï¼‰ã€‚atan2 ã®çµæœã¯ãƒ©ã‚¸ã‚¢ãƒ³ã€‚
                dy = y2 - y1
                dx = x2 - x1
                angle_rad = np.arctan2(dy, dx)
                angle_deg = np.degrees(angle_rad)
                # æ­£è¦åŒ–ã—ã¦ 0..180 ã®ç¯„å›²ã«ã™ã‚‹ï¼ˆçµ¶å¯¾è§’åº¦ï¼‰
                abs_angle = abs(angle_deg)
                if abs_angle > 180:
                    abs_angle = abs_angle % 180

                # è‰²åˆ†ã‘: -5..5 åº¦ (ã»ã¼æ°´å¹³) -> èµ¤, 85..95 åº¦ (ã»ã¼å‚ç›´) -> ç·‘, ãã‚Œä»¥å¤– -> ç°
                # è§’åº¦ã¯ signed ã ãŒã»ã¨ã‚“ã©æ°´å¹³åˆ¤å®šã¯ abs(angle) <= 5 ã¨ã—ã¦æ‰±ã†
                color = (192, 192, 192)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç° (BGR)
                # æ°´å¹³ä»˜è¿‘ï¼ˆ-5ã€œ5åº¦ï¼‰
                if -5.0 <= angle_deg <= 5.0:
                    color = (0, 0, 255)  # èµ¤ (B,G,R)
                # å‚ç›´ä»˜è¿‘ï¼ˆ85ã€œ95åº¦ ã¾ãŸã¯ -95ã€œ-85ï¼‰
                elif 85.0 <= abs_angle <= 95.0:
                    color = (0, 255, 0)  # ç·‘

                # cv2.line(overlay, (x1, y1), (x2, y2), color, 2)

        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: æ¤œå‡ºçµæœï¼ˆå››è§’ + ç›´ç·šï¼‰ã‚’ä¿å­˜
        if self.debug_mode:
            detected_filename = os.path.join(
                subject_dir, f"capture_{ts}_2_detected.jpg"
            )
            cv2.imwrite(detected_filename, overlay)

        # æç”»å…¥ã‚Šã®ç”»åƒã‚’ãã®ã¾ã¾æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã«æ¸¡ã™ï¼ˆãƒˆãƒªãƒŸãƒ³ã‚°ã¯ã—ãªã„ï¼‰
        processing_frame = overlay

        # ã‚¹ãƒ†ãƒƒãƒ—3: å›è»¢è£œæ­£ã‚’å®Ÿè¡Œï¼ˆãƒˆãƒªãƒŸãƒ³ã‚°å¾Œã®ç”»åƒã«å¯¾ã—ã¦ï¼‰
        # ãƒˆãƒªãƒŸãƒ³ã‚°å¾Œã®ç”»åƒã§ãƒãƒ¼ã‚«ãƒ¼ã‚’å†æ¤œå‡º
        gray_trimmed = cv2.cvtColor(processing_frame, cv2.COLOR_BGR2GRAY)
        corners_trimmed, ids_trimmed, _ = self.detector.detectMarkers(
            gray_trimmed)

        if corners_trimmed is not None and len(corners_trimmed) > 0:
            marker_angle = calculate_marker_rotation(corners_trimmed)
            rotated_frame, rotation_applied = correct_rotation(
                processing_frame, marker_angle
            )
        else:
            # ãƒãƒ¼ã‚«ãƒ¼ãŒæ¤œå‡ºã§ããªã„å ´åˆã¯å›è»¢è£œæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—
            rotated_frame = processing_frame
            rotation_applied = 0.0

        if self.debug_mode:
            rotated_filename = os.path.join(
                subject_dir, f"capture_{ts}_3_rotated.jpg")
            cv2.imwrite(rotated_filename, rotated_frame)

        # ã‚¹ãƒ†ãƒƒãƒ—4: å›è»¢å¾Œã®ç”»åƒã«å¯¾ã—ã¦ãƒ›ãƒ¯ã‚¤ãƒˆãƒãƒ©ãƒ³ã‚¹è£œæ­£ã‚’é©ç”¨
        if self.white_balance_enabled:
            # å›è»¢å¾Œã®ç”»åƒã§ãƒãƒ¼ã‚«ãƒ¼ã‚’å†æ¤œå‡º
            gray_rotated = cv2.cvtColor(rotated_frame, cv2.COLOR_BGR2GRAY)
            corners_rotated, ids_rotated, _ = self.detector.detectMarkers(
                gray_rotated)

            if corners_rotated is not None and len(corners_rotated) > 0:
                corrected_frame, viz_info, white_bgr, black_bgr = auto_white_balance(
                    rotated_frame, corners_rotated
                )
            else:
                # ãƒãƒ¼ã‚«ãƒ¼ãŒæ¤œå‡ºã§ããªã„å ´åˆã¯å›è»¢å¾Œã®ç”»åƒã‚’ãã®ã¾ã¾ä½¿ç”¨
                corrected_frame = rotated_frame
                viz_info, white_bgr, black_bgr = None, None, None
        else:
            corrected_frame = rotated_frame
            viz_info, white_bgr, black_bgr = None, None, None

        if self.debug_mode:
            wb_filename = os.path.join(
                subject_dir, f"capture_{ts}_4_white_balance.jpg")
            cv2.imwrite(wb_filename, corrected_frame)

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆã—ã¦æœ€çµ‚ç”»åƒã‚’ä¿å­˜
        filename = os.path.join(subject_dir, f"capture_{ts}.png")
        cv2.imwrite(filename, corrected_frame)

        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ã‚°ãƒªãƒƒãƒ‰ä»˜ãã®ç”»åƒã‚‚ä¿å­˜
        if self.debug_mode and viz_info is not None:
            debug_frame = draw_debug_grid(corrected_frame, viz_info)
            debug_filename = os.path.join(
                subject_dir, f"capture_{ts}_5_grid.png")
            cv2.imwrite(debug_filename, debug_frame)

        # show the saved image in the video_label
        # Try to load with QImage first. However, QImage may return a null image
        # if the image is too large (Qt allocation limits). Detect that case and
        # fall back to OpenCV-based conversion which is more robust here.
        image = QtGui.QImage(filename)
        if image.isNull():
            # Fallback: convert with OpenCV -> QImage from buffer
            try:
                rgb = cv2.cvtColor(corrected_frame, cv2.COLOR_BGR2RGB)
                h, w, ch = rgb.shape
                bytes_per_line = ch * w
                qimg = QtGui.QImage(
                    rgb.data, w, h, bytes_per_line, QtGui.QImage.Format.Format_RGB888
                )
                pix = QtGui.QPixmap.fromImage(qimg)
            except Exception:
                pix = QtGui.QPixmap()
        else:
            pix = QtGui.QPixmap.fromImage(image)

        # Ensure pixmap is valid before scaling/setting to avoid QPixmap::scaled null warnings
        if not pix.isNull():
            pix = pix.scaled(
                self.video_label.size(), QtCore.Qt.AspectRatioMode.KeepAspectRatio
            )
            self.video_label.setPixmap(pix)
            # Store the paused display frame
            self.paused_display_frame = corrected_frame.copy()
        else:
            # As a last resort, show nothing but log a warning
            print(
                "Warning: failed to create a valid QPixmap for display (image may be too large)."
            )

        perspective_info = "\nå°å½¢è£œæ­£: é©ç”¨" if perspective_applied else ""
        rotation_info = (
            f"\nå›è»¢è£œæ­£: {rotation_applied:.1f}åº¦"
            if abs(rotation_applied) >= 1.0
            else ""
        )
        toast_msg = f"æ•™ç§‘: {subject_name}\nãƒãƒ¼ã‚«ãƒ¼ID: {marker_id}{perspective_info}{rotation_info}\nä¿å­˜å®Œäº†"
        toast = ToastNotification(toast_msg, self, duration=4000)
        toast.show()

        # YomiTokuã®å‡¦ç†ã‚’éåŒæœŸã§å®Ÿè¡Œ
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨æ•™ç§‘ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¿å­˜ã—ã¦ã€ãƒ¯ãƒ¼ã‚«ãƒ¼å®Œäº†æ™‚ã«ä½¿ç”¨
        self.current_ts = ts
        self.current_subject_dir = subject_dir
        self.current_corrected_frame = corrected_frame.copy()

        yomitoku_worker = YomiTokuWorker(frame=corrected_frame, parent=self)
        yomitoku_worker.finished.connect(self.on_yomitoku_result)
        yomitoku_worker.error.connect(self.on_yomitoku_error)
        yomitoku_worker.start()

        # pause camera feed by setting a flag instead of stopping timer
        # stopping/starting the timer causes MJPEG stream sync issues
        self.camera_paused = True

    def on_yomitoku_result(
        self, results: "yomitoku.schemas.OCRSchema", ocr_vis, layout_vis
    ):
        """YomiTokuã®å‡¦ç†ãŒå®Œäº†ã—ãŸæ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        ts = self.current_ts
        subject_dir = self.current_subject_dir
        corrected_frame = self.current_corrected_frame

        # HTMLå½¢å¼ã§è§£æçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆresults ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        if results is not None:
            try:
                print(type(results))

                json_filename = os.path.join(
                    subject_dir, f"capture_{ts}_analysis.json")

                results.to_json(json_filename, img=corrected_frame)

                # wordsã‹ã‚‰å„contentã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã§çµåˆ
                ocr_text = "\n".join(word.content for word in results.words)
                print(ocr_text)

                self.last_ocr_text = ocr_text
            except Exception as e:
                print(f"Warning: failed to export analysis to HTML: {e}")

        # å¯è¦–åŒ–ç”»åƒã‚’ä¿å­˜ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        if ocr_vis is not None:
            try:
                ocr_filename = os.path.join(
                    subject_dir, f"capture_{ts}_ocr_vis.jpg")
                cv2.imwrite(ocr_filename, ocr_vis)
            except Exception as e:
                print(f"Warning: failed to save ocr_vis: {e}")
        if layout_vis is not None:
            try:
                layout_filename = os.path.join(
                    subject_dir, f"capture_{ts}_layout_vis.jpg"
                )
                cv2.imwrite(layout_filename, layout_vis)
            except Exception as e:
                print(f"Warning: failed to save layout_vis: {e}")

    def on_yomitoku_error(self, error_msg):
        """YomiTokuã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸæ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print(f"Warning: YomiToku processing failed: {error_msg}")

    def resume_camera(self):
        # resume live feed by clearing the paused flag
        self.camera_paused = False
        self.paused_display_frame = None

    def open_ai_processing(self):
        """AIå‡¦ç†ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹ã"""
        if not self.last_ocr_text.strip():
            QtWidgets.QMessageBox.information(
                self,
                "æƒ…å ±",
                "å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚\nå…ˆã«ç”»åƒã‚’æ’®å½±ã—ã¦OCRã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
            )
            return

        # AIå‡¦ç†ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’è¡¨ç¤º
        dialog = AIProcessingDialog(
            self, self.last_ocr_text, self.last_subject_name)
        dialog.exec()

    def show_ai_page(self):
        """ã‚«ãƒ¡ãƒ©ç”»é¢ã‹ã‚‰AIç”»é¢ã¸é·ç§»ã™ã‚‹ã€‚captures ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
        # ã‚«ãƒ¡ãƒ©è¡¨ç¤ºã‚’ä¸€æ™‚åœæ­¢ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿å–ã‚Šã¯ç¶šã‘ã‚‹ï¼‰
        self.camera_paused = True
        # ã‚¹ã‚¿ãƒƒã‚¯å†…ã®ãƒšãƒ¼ã‚¸ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹
        try:
            self.stack.setCurrentWidget(self.ai_page)
            # ãƒœã‚¿ãƒ³ã®çŠ¶æ…‹ã‚’æ›´æ–°
            self.camera_mode_btn.setChecked(False)
            self.ai_mode_btn.setChecked(True)
            # captures ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å±¥æ­´ã‚’èª­ã¿è¾¼ã‚“ã§è¡¨ç¤º
            self.load_capture_history()
        except Exception as e:
            print(f"Warning: failed to switch to AI page: {e}")
            return

    def show_camera_page(self):
        """AIç”»é¢ã‹ã‚‰ã‚«ãƒ¡ãƒ©ç”»é¢ã¸æˆ»ã™ã€‚"""
        # ã‚¹ã‚¿ãƒƒã‚¯å†…ã®ãƒšãƒ¼ã‚¸ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹
        try:
            self.stack.setCurrentWidget(self.camera_central)
            # ãƒœã‚¿ãƒ³ã®çŠ¶æ…‹ã‚’æ›´æ–°
            self.camera_mode_btn.setChecked(True)
            self.ai_mode_btn.setChecked(False)
        except Exception as e:
            print(f"Warning: failed to switch to camera page: {e}")
            return
        # ã‚«ãƒ¡ãƒ©è¡¨ç¤ºã‚’å†é–‹
        self.camera_paused = False

    def load_capture_history(self):
        """captures ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰æ’®å½±å±¥æ­´ã‚’èª­ã¿è¾¼ã‚“ã§ãƒªã‚¹ãƒˆè¡¨ç¤ºã™ã‚‹ã€‚"""
        self.capture_list.clear()
        self.subject_filter.clear()

        # å…¨æ•™ç§‘ã‚’å–å¾—
        subjects = set(["ã™ã¹ã¦"])
        capture_items = []

        if not os.path.exists(self.captures_dir):
            return

        # æ•™ç§‘ã”ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        for subject_name in os.listdir(self.captures_dir):
            subject_path = os.path.join(self.captures_dir, subject_name)
            if not os.path.isdir(subject_path):
                continue

            subjects.add(subject_name)

            # å„æ•™ç§‘ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
            for filename in os.listdir(subject_path):
                if filename.endswith('.png') and not any(x in filename for x in ['_ocr_vis', '_layout_vis', '_grid', '_original', '_perspective', '_detected', '_rotated', '_white_balance']):
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡º (capture_20231103_123456.png)
                    if filename.startswith('capture_') and len(filename) > 20:
                        timestamp_str = filename[8:23]  # 20231103_123456
                        try:
                            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ‘ãƒ¼ã‚¹
                            dt = datetime.strptime(
                                timestamp_str, "%Y%m%d_%H%M%S")

                            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                            json_path = os.path.join(
                                subject_path, f"capture_{timestamp_str}_analysis.json")
                            has_ocr = os.path.exists(json_path)

                            capture_items.append({
                                'subject': subject_name,
                                'timestamp': dt,
                                'timestamp_str': timestamp_str,
                                'image_path': os.path.join(subject_path, filename),
                                'json_path': json_path if has_ocr else None,
                                'display_text': f"{subject_name} - {dt.strftime('%Y/%m/%d %H:%M:%S')}"
                            })
                        except ValueError:
                            continue

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        capture_items.sort(key=lambda x: x['timestamp'], reverse=True)

        # æ•™ç§‘ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨­å®š
        self.subject_filter.addItems(sorted(subjects))
        self.subject_filter.setCurrentText("ã™ã¹ã¦")

        # ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰
        self.all_capture_items = capture_items
        self.filter_captures_by_subject("ã™ã¹ã¦")

    def filter_captures_by_subject(self, subject):
        """æ•™ç§‘ã§æ’®å½±å±¥æ­´ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€‚"""
        self.capture_list.clear()

        if not hasattr(self, 'all_capture_items'):
            return

        for item in self.all_capture_items:
            if subject == "ã™ã¹ã¦" or item['subject'] == subject:
                list_item = QtWidgets.QListWidgetItem(item['display_text'])
                list_item.setData(QtCore.Qt.ItemDataRole.UserRole, item)
                self.capture_list.addItem(list_item)

    def on_capture_selected(self):
        """ãƒªã‚¹ãƒˆã§é¸æŠã•ã‚ŒãŸæ’®å½±å±¥æ­´ã®è©³ç´°ã‚’è¡¨ç¤ºã€‚"""
        current_item = self.capture_list.currentItem()
        if not current_item:
            return

        item_data = current_item.data(QtCore.Qt.ItemDataRole.UserRole)
        if not item_data:
            return

        # ç”»åƒã‚’è¡¨ç¤º
        image_path = item_data['image_path']
        if os.path.exists(image_path):
            pixmap = QtGui.QPixmap(image_path)
            if not pixmap.isNull():
                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ©ãƒ™ãƒ«ã®ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaled_pixmap = pixmap.scaled(
                    self.preview_label.size(),
                    QtCore.Qt.AspectRatioMode.KeepAspectRatio,
                    QtCore.Qt.TransformationMode.SmoothTransformation
                )
                self.preview_label.setPixmap(scaled_pixmap)
            else:
                self.preview_label.setText("ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            self.preview_label.setText("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # OCRçµæœã‚’è¡¨ç¤º
        json_path = item_data.get('json_path')
        if json_path and os.path.exists(json_path):
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # words ã‹ã‚‰ content ã‚’æŠ½å‡º
                    if 'words' in data:
                        ocr_text = '\n'.join(word.get('content', '')
                                             for word in data['words'])
                        self.ai_text_display.setPlainText(ocr_text)
                        # AIå‡¦ç†ç”¨ã«ä¿å­˜
                        self.last_ocr_text = ocr_text
                        self.last_subject_name = item_data['subject']
                    else:
                        self.ai_text_display.setPlainText("OCRçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            except Exception as e:
                self.ai_text_display.setPlainText(f"OCRçµæœã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            self.ai_text_display.setPlainText("OCRçµæœãŒã‚ã‚Šã¾ã›ã‚“")
            self.last_ocr_text = ""

    def closeEvent(self, event):
        self.timer.stop()
        # stop ocr timer if present (older versions may have created it)
        if getattr(self, "ocr_timer", None):
            try:
                self.ocr_timer.stop()
            except Exception:
                pass
        try:
            if self.cap and self.cap.isOpened():
                self.cap.release()
        except Exception:
            pass
        event.accept()


def main():
    app = QtWidgets.QApplication(sys.argv)

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’å–å¾—
    debug_mode = "--debug" in sys.argv or "-d" in sys.argv

    win = CameraWindow(debug_mode=debug_mode)

    if debug_mode:
        print("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã—ã¾ã—ãŸã€‚ã‚°ãƒªãƒƒãƒ‰ä»˜ãç”»åƒã‚‚ä¿å­˜ã•ã‚Œã¾ã™ã€‚")

    win.show()
    sys.exit(app.exec())


if __name__ == "__main__":
    main()
